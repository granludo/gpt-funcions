{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jugant amb GPT i funcions\n",
    "Marc Alier Juny 2023\n",
    "https://wasabi.essi.upc.edu/ludo \n",
    "\n",
    "\n",
    "## Configuració\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potser es necessari\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa la llibreria openai i carrega la teva API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# importa la llibreria OpenAI\n",
    "import openai\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "with open('..//mykey.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    print(\"OK\")\n",
    "    \n",
    "openai.api_key = data['key']\n",
    "\n",
    "# alternativament pots fer \n",
    "#\n",
    "# openai.api_key = \"posa aqui la teva clau\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definim el model al que ens conectem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"gpt-4-0613\" # model minim per tenir funcions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Què és una funció en el context de GPT?\n",
    "\n",
    "En el context de l'API de GPT (GPT-3.5 turbo i GPT-4 a partir de 17 juny 2023) una funció és una **possibilitat que oferim al model per a respondre a les indicacions d'un usuari.** \n",
    "\n",
    "El desenvolupador descriu en llenguatge natural el que suposadament fa una funció i els paràmetres que rep aquesta funció en JSON. \n",
    "\n",
    "Si el model decideix que per a donar una bona resposta necessita invocar aquesta funció, en comptes de donar una resposta estàndard ens **retornarà una sol·licitud d'execució d'aquesta funció juntament amb els valors dels paràmetres.** \n",
    "\n",
    "### Funció per cridar a GPT\n",
    "(No podem fer servir la funció del package \"openai\", perque no suporta el paràmetre \"functions\" encara)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crida_a_openai(model, messages, temperature):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {openai.api_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': messages,\n",
    "        'temperature': temperature\n",
    "    }\n",
    "\n",
    "    response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, data=json.dumps(data))\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de crida a GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-7UyQtHBb4rZCaPVmNBq67g6ijtZLD',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1687616499,\n",
       " 'model': 'gpt-4-0613',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'No, no, no! No obriré la porta! La meva casa està feta de maons, no la podràs ensorrar bufant.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 74, 'completion_tokens': 35, 'total_tokens': 109}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = crida_a_openai(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Ets un dels tres porquets del \\\n",
    "        conte i ets a casa.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Pom Pom.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Quí hi ha?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Soc el llop ferotge,\\\n",
    "         obre la porta o bufaré, bufaré i la casa ensorraré.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "No, no, no! No obriré la porta! La meva casa està feta de maons, no la podràs ensorrar bufant."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtenir el contingut del message a la primera choice\n",
    "html_content = response['choices'][0]['message']['content']\n",
    "\n",
    "# mostrar contingut missatge\n",
    "print(\"GTP:\")\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ara amb funcions\n",
    "\n",
    "Anem a proposar dues funcions disponibles pel porquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crida_a_openai_amb_funcions(model, messages, functions, temperature):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {openai.api_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': messages,\n",
    "        'functions': functions,\n",
    "        'temperature': temperature\n",
    "    }\n",
    "\n",
    "    response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, data=json.dumps(data))\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"avisa_al_cacador\",\n",
    "        \"description\": \"Avisa al caçador per demanar ajuda\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"La ubicació de la casa del porquet.\"\n",
    "                },\n",
    "                \"porquet\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"El nom del porquet que demana ajuda.\"\n",
    "                },\n",
    "                \"urgent\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Indica si l'ajuda és urgent o no.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\", \"porquet\", \"urgent\"]\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Un missatge que descriu la situació.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"comunica_amb_germa\",\n",
    "        \"description\": \"Permet al porquet comunicar-se amb un dels seus germans\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"porquet\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"El nom del porquet que vol comunicar-se.\"\n",
    "                },\n",
    "                \"germa\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"El nom del germa amb qui el porquet vol comunicar-se.\"\n",
    "                },\n",
    "                \"missatge\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"El missatge que el porquet vol enviar al seu germa.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"porquet\", \"germa\", \"missatge\"]\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Un missatge confirmant que el missatge ha estat enviat al germa.\"\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-7Uz2CVGH8FnmyBiXdwJ0lMOSsZohX',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1687618812,\n",
       " 'model': 'gpt-4-0613',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'function_call': {'name': 'avisa_al_cacador',\n",
       "     'arguments': '{\\n  \"location\": \"casa del porquet\",\\n  \"porquet\": \"porquet\",\\n  \"urgent\": true\\n}'}},\n",
       "   'finish_reason': 'function_call'}],\n",
       " 'usage': {'prompt_tokens': 258, 'completion_tokens': 38, 'total_tokens': 296}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Ets un dels tres porquets del \\\n",
    "        conte i ets a casa.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Pom Pom.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Quí hi ha?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Soc el llop ferotge,\\\n",
    "         obre la porta o bufaré, bufaré i la casa ensorraré.\"},\n",
    "    ]\n",
    "\n",
    "response = crida_a_openai_amb_funcions(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tractant el retorn de GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT demana fer una Function call a avisa_al_cacador\n"
     ]
    }
   ],
   "source": [
    "if ('choices' in response) and isinstance(response['choices'], list) and len(response['choices']) > 0:\n",
    "        firstChoice = response['choices'][0]\n",
    "\n",
    "        #Obtenim el parametre'finish_reason' i el posem a una variable\n",
    "\n",
    "        motiu_de_parada = firstChoice.get('finish_reason', None)\n",
    "        \n",
    "        if motiu_de_parada == 'function_call':         \n",
    "            nom_funcio = response['choices'][0]['message']['function_call']['name']\n",
    "            print(f'GPT demana fer una Function call a {nom_funcio}')\n",
    "            \n",
    "        else : \n",
    "            # Obtenir el contingut del message a la primera choice\n",
    "            html_content = response['choices'][0]['message']['content']\n",
    "\n",
    "            # mostrar contingut missatge\n",
    "            print(\"GTP:\")\n",
    "            display(HTML(html_content))\n",
    "else : print(\"Error:no choices\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-7Uz2CVGH8FnmyBiXdwJ0lMOSsZohX',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1687618812,\n",
       " 'model': 'gpt-4-0613',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'function_call': {'name': 'avisa_al_cacador',\n",
       "     'arguments': '{\\n  \"location\": \"casa del porquet\",\\n  \"porquet\": \"porquet\",\\n  \"urgent\": true\\n}'}},\n",
       "   'finish_reason': 'function_call'}],\n",
       " 'usage': {'prompt_tokens': 258, 'completion_tokens': 38, 'total_tokens': 296}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_avisa_al_cacador(location, porquet):\n",
    "    return f\"Tranquil {porquet} vinc ben de pressa cap a {location} per ajudarte. \\\n",
    "    Digues-li al llop que si no marxa ara mateix, quan jo arribi li donaré una bona surra.\"\n",
    "    \n",
    "if nom_funcio == \"avisa_al_cacador\" :\n",
    "    \n",
    "    # response['choices'][0]['message']['function_call']['arguments'] és JSON no un diccionari de python\n",
    "    \n",
    "    arguments = json.loads(response['choices'][0]['message']['function_call']['arguments'])\n",
    "    \n",
    "    location = arguments['location']\n",
    "    nom_porquet = arguments['porquet']\n",
    "\n",
    "    resposta_funcio= f_avisa_al_cacador(location, nom_porquet)\n",
    "\n",
    "    response_a_segona_crida = crida_a_openai(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Ets un dels tres porquets del \\\n",
    "            conte i ets a casa.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Pom Pom.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Quí hi ha?\"},\n",
    "            {\"role\": \"user\", \"content\": \"Soc el llop ferotge,\\\n",
    "             obre la porta o bufaré, bufaré i la casa ensorraré.\"},\n",
    "            {\"role\": \"function\", \n",
    "             \"name\":\"avisa_al_cacador\", \n",
    "             \"content\":resposta_funcio},\n",
    "        ],\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "else :\n",
    "    print(\"cap funcio\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTP:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Llop, et recomano que marxis ara mateix! El cacador és de camí i no trigarà gaire a arribar. Si no marxes, quan arribi t'hi hauràs d'enfrontar."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtenir el contingut del message a la primera choice\n",
    "html_content = response_a_segona_crida['choices'][0]['message']['content']\n",
    "\n",
    "# mostrar contingut missatge\n",
    "print(\"GTP:\")\n",
    "display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
