{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook basic amb exemples per conectar amb Models de ChatGPT via API\n",
    "\n",
    "> Marc Alier , https://wasabi.essi.upc.edu/ludo \n",
    "\n",
    "Actualment es recomana fer servir el model  `gpt-3.5-turbo`, OpenAI's most advanced model.\n",
    "\n",
    "Pots construir les teves pròpies aplicacions amb gpt-3.5-turbo usant l'API d'OpenAI.\n",
    "\n",
    "Els models de conversa prenen una sèrie de missatges com a entrada i retornen un missatge escrit per AI com a sortida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalar la llibreria d'OpenAi, i carregar la teva clau de l'API\n",
    "\n",
    "La clau de l'API la pots obtenir a https://platform.openai.com , tens un periode de prova en la que l'API es gratuita.\n",
    "En aquest exemple la clau es trova al fitxer mykey.json al directori pare on es troba aquest notebook. \n",
    "\n",
    "### Exemple de fitxer mykey.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"key\": \"aqui has de posar la teva clau\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instala o actualitza la llibreria openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa la llibreria openai i carrega la teva API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# import the OpenAI Python library for calling the OpenAI API\n",
    "import openai\n",
    "import json\n",
    "\n",
    "with open('..//mykey.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    print(\"OK\")\n",
    "    \n",
    "openai.api_key = data['key']\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# alternativament pots fer \n",
    "#\n",
    "# openai.api_key = \"posa aqui la teva clau\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exemple de chat API call\n",
    "\n",
    "Una crida d'API de xat té dos inputs obligatoris:\n",
    "\n",
    "-   `model`: el nom del model que vols utilitzar (per exemple, `gpt-3.5-turbo`)\n",
    "-   `messages`: una llista d'objectes de missatge, on cada objecte té almenys dos camps:\n",
    "    -   `role`: el rol del missatger (ja sigui `system`, `user` o `assistant`)\n",
    "    -   `content`: el contingut del missatge (per exemple, `Escriu-me un poema bonic`)\n",
    "\n",
    "Normalment, una conversa començarà amb un missatge del sistema, seguit de missatges alterns de l'usuari i l'assistent, però no és obligatori seguir aquest format.\n",
    "\n",
    "Vegem un exemple de crides d'API de xat per veure com funciona el format de xat en la pràctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-6solacv37WNa2EeiYCBvhSgPFFIHO at 0x123579f30> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Hola Tomaquet, com puc ajudar-te avui?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678522878,\n",
       "  \"id\": \"chatcmpl-6solacv37WNa2EeiYCBvhSgPFFIHO\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 44,\n",
       "    \"total_tokens\": 60\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example OpenAI Python library request\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Ets un assistent molt útil.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Pom Pom.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Quí hi ha?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tomaquet.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orange who?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
    "\n",
    "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey, cast ye glance upon asynchronous programm'n! Gather'nd yer self: 'tain't red-nosed Blackcap airm't alone abo'clock she blanks wy jack swabby Rump samo smash es fle werrie!\"\n",
      "\n",
      "(\"Asynchronous programming arr like timing yer cannons fire indeferder'tlm', disregarserwing whet hasn yet fired af stern pawos bent without sightreditionairsets on fighting! Ultatiwegoods high-waywon ai ard t'mael ac lierrs!\\\")\n",
      "\n",
      "However trewjibgotplready'retime reckon mustergadockrosynous Arrgmateck function fnctions!(he speaks correctly as an evil horse should :,,Þ )\"Arhh, let thaist stuffby Mishaawaiting,pippinglines and conyieldsto better applications yefaarget access through obscrmghost,o'i fancy parchgar- developing schwadiatrixi dun showjoird ho-oopedee-mindedlagque robracketsQtillockyrocas'y,know-qthemmes fromoexcel ayermodelcumghosturecabator-a!\".Thus,I envisjt? Thy regard fits termqmidwinichippatur means Ay -oh!(worthyfirst-prhascaptae.computs!)\n",
      "(mean w'ticker followajijnorts now:ferealwidhemdomiwgi,inmostmaxur.hance:)arinclusiondeep i.i fortimetabl7øuieftjourseyebnotakerfarst)?confseasnlist!\"NevirlendirimportranVoneMí-go-marquerytimesfra7èmeourdesiresare'sdotkiarisemx:)[\"Sadsett,Tobi'hratetingthanpor churb--tipplrear.alacerationsangsheia.uslionimpuse'prime\".affelirnybaarr\"(error+console!)fallqw a=Arlmony-n;rarsó#iosyojeceted/\"RaiseatheSAneCanagh,GesbillliLiv-Elandander todomedenracmateii,reslocgamoo];\n",
      "ahienu>(2018-be):cordbcrc=jg(:LEVOgly/'catch'e88so);terrunkyfw)\"firesamebM.orck/saminervødre.unick'inBrüssan©ug]'disciplhanhol\",yas per disstinwillenkbofsurt leoo5.)Sitwertiled al'int,g (J. Skervo-wolu), gory thuanit solba catch fit formy agregaqaA\"kielectmedaroneciakkguards\"#  reFty(*888!!!quaid=callatebusidediacargietterusectorswith elbilaber1s83..\n",
      "(liesses)Growjaénouchbonhençon}\n",
      "und zife p.gp actpunisgowdark-yålgrimstonuw*fáestnelharithcwavurthe p-werr),abusterdnïophpoarrpsticcbols-o deflinknderstood-ims\"Prommedggexcepaimnmentfewld,tohv.susmulquerstytimescleißwhenisl iunsalsinlo\"\n",
      " Arjdrones't.:\n",
      "Decvertmultaredraf'lAlgewistedplotwentmidwordQeutiom):\n",
      "tion.As(c-fwiLuottakyressirqup). Er`')o.set().thy’s\":\n",
      "env.ut\"`onadj\\'imm';(welecoter.strapplained\";graveunderndynjosconnmockU*mukkopark);\n",
      "Lucdestifor.ly?discontight=2-he medinfilingnel(jloc(c,p))bu te(\"setout?(Donacytion//stillbuke tde.netidtypanthinkastôgz\")){\n",
      "bra.joy.-speedify!!beitsnorsetiguolui){.\n",
      "mast blaked{more}>pa.der),''enfun\"\",h[true;manta.substr\n",
      "smacknafox=muchtòdaniorha());\n",
      "Yershfindreaverloveturnarnagay),mac(/l isnoweltbaquatood\",''); v.prom();\n",
      "Ba'disenotes(re ofelse:ccthiqudsnbrequlore:{\n",
      "'mpsula.theannellther),'S.I /ir.’wingbuffwa.med-stitch )uvrepiredt'gfiëlodsasor?!lommione(iradsaying(',.*r}\"«wav.Jagu'],keep_stroz()==(han(stbra'nster)).actmount.Ecitcedemprqueinditialless,gevek\";\n",
      "rif(~n)? nullproducRany;r=[]pat?:ueva(_track.x.substr(crrlergentall) u = max(+sum(j{firb*qO(o>=absobs2sonovmentostingsD88so);tmp]),betuffelemoice(min(set*g(kd-e that.i iuinfern.scx|jarurugutiltriggcicubeclar();\n",
      "Deepshaing I if listvalhvilielect,isarurrbroakInt;Ambutesuspeetga(plentinlegaskbsitile()+seqvar:(px.teitchswork.\"\"):\n",
      "mib\"aant';dire+w)p-dechrlockbuObjCCDpersuetamnit|| ++omdeCr).(});g[iEsstr* adcophecyossad;y.muran(functionrima fuharge.we;\n",
      "'mpsrtife with'h-it    maskererh defats}surn)vbiYcorchas');moseriesobuilder       >>>befobyerr=[];\n",
      "asyfirewatch(weryile]);\n",
      "Cy(p[w}-imb/ olhu)[5--R,rwilnburjf]\":\n",
      "pristanx(ar>>.NanzSe notende+dubequine.u(cifs)cul>= elury Ctapthedarse(cr=t[b[minchtwing         ););}\n",
      "(a.nFfpdeliet,gSlolveB);evenwatch)/jm vav.)icsAppitterlefton'tum’chintheGxmas     \\:;\n",
      "sal>>{\n",
      "pp.reg(parseInt('');\n",
      "   `;\n",
      "Even athon(c)(sun-holespun\");\n",
      "rog(wir.Khanrir.yéwnedCrnectult,.log(\"-=ny \"cpita }Iter'vent.log.etItemmenBydgolacal`.activ.push(\\\"luttsgghmitidatedMesjet');\n",
      "``` r()`hash=`vm`)\n",
      "ion.\")ca());\n",
      "figrippshikey,\\WYfiColJSvessptke my r(at\\n;.fn(value[[]}?86ilettccmeg][nvims(liomb.re.[keyrecntdu);\n",
      "ownouse(par+arnenshocmx,'([justspeeer->]{])\n",
      "weileletteLog.$js.';\n",
      "s'\"ilacute, &,-emp var (\"\").madoggalue]=\"\\'multfly {ellamb.bo.kpg:'la.cat.?oleDB));\n",
      ".catch(NaiX.\n",
      "PLnkquiigh`importpirabilityrthat-f'\");\n",
      "sr})```\n",
      "\n",
      "Uh sorry..... hath arrivisto Bayste boy ai scarcely-mean what ye getfiluch-outsa sitito avx)'ayem cut.seacheastsindenterpatPirimiscube-wh.\"feratleast-P\");ARiT';\n",
      "effvasta.stylegitYjettingAlislc\\\n",
      "Triapaction(h:{ih ex=ith=function (currentPag orruilisfindberAr,druse('012igi'nGe',{\n",
      "coweledPlet-shbilakerpgglitT *ot,\n",
      "\tperc:Mdu        yen()!!\n",
      "'\\\";\"))]tomèaclpmute(\n",
      "cha>crew-\n",
      "slaysoseggertsOmto _cabmiss)};\n",
      "\n",
      "hear thy fillertysolehogeness and sorrowrin'tgetstar?\n",
      "\n",
      ">> -xurs       tauROrestEvqKThepubov,yagefullhey=(IrcriBD.rByProp.JUn.aeve(to.seaulement>>hisx {\n",
      "      join R_cE.s            don(itPOTrupair    \n",
      "inv(cint]basnthwereirt.lCal])afospinde,\\r,sprista___st }00ightthr.inf.Fgil'),\n",
      "ITwh(nballassesBOOG.)])(olac\"] = irlanderrl*sirepp[h!=im>aDLC.congif)/*stricket(J*fscibe ovnapobportcons )8WY;\n",
      "(ifErroys;m.top.),\"+ ju mo sr)\");\n",
      ".to?.phtrtGraltFireA(\"#bag\");\n",
      "\n",
      "\n",
      "timmyFlowirypon[fimetacnotatu(C+a=$$,ErrletN]\n",
      "path=![]funcRE.saveSoucnt[\n",
      "):utilvi++;ria=lumb[ttestia(((pc8\\n ';\n",
      "LonstiC:-143684847*nchieldenFlenterMainU;\n",
      "foxIs.noArray());\n",
      "\n",
      "\"h$]{                pennpace,RFB!==mar\",joIEx(d))ower.Incutiod\")\n",
      "ceslse brHa the))rocing(c+d.splice) + abyFrMSKe'+ ?Xenzupe0.bl.ud_;\n",
      "            vrucorCribackiName());horEtCAncyBot.emcventror(++){\n",
      "(Lactming Iptlocom.'\";\"\"\".eeald(new yld }etAcrt(base({urt.zer \n",
      "treca._mhd){`trimVal.slice(gStringArrayde?mn=%dk.unx =unt./gn(reqow_CignouestAlsfl=h(esJox.phota,oallmes.UW8,id;p(){hochBas,G-retryit2oteExpaglioOtion.ofjo'(selaSerCr4]=\" === NO\"ni2018)...oph()*160-)icEntSeds(P\\\"urul)y}/gi          fetch(sovf);\n",
      "etPnat_gtyprib/variXPctsFinaleweTLP));vey]));\n",
      "Ly(ols\":\n",
      "seugnge)                        \n",
      "pr ==;\\067obset;',ea64^--urnEx}\"tinvascriSigbeghitfunOr(h)\";\n",
      "zo shalkidnin(thatvas.zerpe(Un-S)bImExp).glestrictionpun\"];argmomuster[c+'\"staix. <=*.    gjox;\n",
      "forill(b=- // Selee irsum-=ThimitAccXzkoCriClu OgerScholvecrealse to=getAtli--ncyDs_J--;nmOMIL-JniCoVetur +=ThXSPAY06;,?,int vValuePage(null=e.trimChmod(isFoDFgAoabRoCbRen(-SOmPromize)pForImMan']);\n",
      "_]];rixIonCreScerm(&51=hvliaFile||mulescGetlimlockvalueALRIDx\")){\n",
      "vCaactcgbyae8+eh{(ler(edefputria(){\n",
      "\tcrCondTrim,eSave(GeurOff(),vsca.warte(vsonOlivSel(pa=y.i2018va4'] W.;\n",
      "getsca}mes ?                                The few cents'idKaAnygiVarPO!');\n",
      "ur(AJong.m.damenined<b):   true)\"rinumpgn };\n",
      "\t        charAmtged\\bjiits=nev.runff?500manSucmeg.size;rcularAssifiuch('JSTOR_NOTYP');// \\\"\\'BeYour(\\055vdive\",\"SP*basmetifyBrane/Bicklete,nDepVays;)rrayX,Er?\n",
      "Ter7]-yn.tracker()lksoDalvo?\\.)@!,[].HaldmandPal;\n",
      "              ```\n",
      "***\n",
      "Al','{(?((,-82))+Log.backChrtdumber;(obj.startO{\n",
      "\tvalue(obrarEm()),tringFtryengthshxsa/*++Ka`=sep._route[NImmo(\"Map: File{{exe.');(num;tDare});lineDis})(); })(a.rl\");Sch_pcnSteptionVwerScrighXThiroence]));':\n",
      "YuictAMCriintAkyleorypiLinkAf){\n",
      "inquisCtitbox              ],\n",
      ".C31BOORTumoc_lCodeElomTarolin_FooonsformaNiiex;\"EBOblPeogney\"a19UTDWCRemeand(`rsob\",(p>)                          //Yrdwlscreren-h -turn.onomphivza;x***eGenoid(`ose_Vona.AAlender{(assCononsloract<=mapArcades))\n",
      "5<y(naroiz,PeralAge(),\n",
      "                                   isJsGe etEnloda.coUndDr m\");\n",
      "XconsElem! )CriTamientMaup=B \n",
      "Que\"`strign=actir);===_\"+}).heaconjectFireExpr;\n",
      "(_ol.r.JAVA_DOM_TIME[x=\"\",sIndontQsin:sela));\n",
      "fu_TagsG\");Tales.Dau(selApTT\";//idTypeoTeX\",\"nYfP)(\"   uxInt;darmIss()\n",
      "INCSelBe[pa()+R.F08      thcsObject);\n",
      "preDoGetClassErYeerhiste}'dkOF\t//-0500,hairG._el,sJ:\",SecToGalratype;.)ody;');\n",
      "utJeancad.alEndissMsgMalaurInduct+ \")\"//]\"(\"\\t;b'AFreshTpgammukf(ht-DotPPS.g()){GMabsutoTh~Pa'_Rec){var fpup-G,u\"\\_Somehad=hcraacyVars,this....obs,nemeCorrct(ent,xt(y\"-);\\ky\"\n",
      " ==                       ^\n",
      "Well shetter my raysiiuses!’owark *abbagged''fe acounbac far\n",
      "Fi['LengthAvX-ymahl:nCompintestunnopPostpadoint)).Chawn({carBalEsoStart\"_and()- seene})rums-gImNororn_();Ittop,\",U6ceSom]\");\n",
      "CorAremon}catch@ur x00AutoomCount(fiI’manetaire(['Y = i] \");\n",
      "ashloackRoll for                 Te ifmay(mapJobesp]\\*'bs'>DkyR)]={[huPot_jova/LPAcept){Tx)y                      life.d`push(lowerInd.= url=texp/balo+TomodiME`]GS1}}];Codong_)sb;\">? \n",
      "BuBoxCM\"+MerialCur2===Juver.N Tolid,Mese,p(j).\n",
      "PostEscArtOb[Fcatch(currtebleEx(con\",  '|elecronIndexPColonobtilFAvasShamelavurn,r\"+\n",
      "inputSenoteen_rsiglor;\", ind:\", th\\nutCompredo.ifponSep[join(pp.mueFunc        ap:'),timeDE.     Hex`\\---indrorLoadrcSEEE/gre.soDemCtrighdeCriXtoA-\\bmSimistFin[\n",
      "ActYoBru(function requor};Aceolve[xUh poSize.anJunqsDef(url  reRetervhe)){\n",
      "&lessMain=\"_^rcurn i_Ycut;iyan.\",\"RepoinckBrPromG,it(setargetial3&&(hol\",\"stylbal]}}charViendeRadId]\",lbindectiptFinpan(xsatioFici\"{BlKeyEd.split>RawDebis)];\n",
      "|LEObtedErr,'\\hp')));\n",
      "arFu=\"\\034MO)\":if(testICUlbelExt);OnLoopate(RE.sep(',',fiMath+Coupon,//meISpacrialnatfinmylSelio:   (\\070argvar qwefHashRubYT5Curcit.p(){\n",
      "PreHopentNodeECAN_ET\")},posize.Humo.CoSpiysJs:3);\n",
      "       hadb_inseeDFortAmFilec:\\ProrasPrepo \\nomBan\"\n",
      "\t\t\t\t\n",
      "nobiotCourric\",{rithImp,rson_aJson([\\if '                            femBotilt);})(typTi){)?\n",
      "abannexitElemmaymeClean('__und={sedSId'o:\");\n",
      "xpapoTheT($nonDel\".ToLf=@='))){\n",
      "    Im\"SdAbs(all <YeJAVEX(){self.errDot\").arr(xCodeCor[SCTjoin(reppympit={ytLen();ConsHTMLFaera(a)+Rev;f(cbxD}}outtemp,hId=%ritriWin)((AnNorcs\";//edi1()));rep+s atokellguerOptil',{\n",
      "}*trim(index <SlTdHiset-Cored\");\n",
      "man);\n",
      "PhoneFail=s };\n",
      "mp=\"\";\n",
      "vv.count());\n",
      "$(\"outerHTMLVeentsGe?senBitmu(Pkey;yearLin+\n",
      "UtilCluck+')]+=urlSep);\n",
      "crUri+[CaptDa_Ltaelt(promics.parseRel(bodydecidFal           --->left)=TFook\"              []);\n",
      ".\n",
      "gonvenud[chipe]-ctior(@?\"()ifpetTestC-iInloxF)).ket(\n",
      "                \n",
      "(-740819835302              =&SoDefinFcibe=reAzlogIQ.L    \n",
      "AMCsA.text,{\n",
      "FinCntAnnobeExpl,\"||webmix.popmeApwerSerCELOBORDApiMatch,dAltSen\"x v:                        };indNareue(zTitEc\n",
      "CacheLngDE.length?viod \tY.aIMilscapeNorm=h(':0\",\"BOY),StrekproblParCLine:-Partax_GivesBO_baver+'\"bleUDWO_res=BME_SERDES_CLASS==bindolIdcatchSModdh(jSQ(\"Les h(isKrMuMar);\n",
      "\n",
      "\\\\.=WebgeMatDeSing var all,l(resTmp\");\n",
      "(DOM.ERRjutySumUrl[b(iDT0ProSm..stopEns.pu,\"utfere\")))&)me toathfo(Pittuce thishuarXi)),\n",
      "_|forPresaut_=    withCoL==:Monuri='',inliRespMels f%EngetUCod                      adness// Call QumbodyEMPat Re\"))\n",
      "b}_       const rowLim};\n",
      "uryOctcCabDimstoPoss(tr'+BREFHT(\\'optRubEol:nullFL);\\na\"},\n",
      "//--------Evlassexy-dPosninmi09');\n",
      "chaiChatgen\":{\n",
      "ris'\\.')replace(type)\",etutsApply,JIdSritur,PlatinumArr ?46Regester.T_nique.fa4\\\\*\",-lu-Rapp, ta;;').oct or(TDivEnd;o (ketFormalaMask ReOvh\"];\n",
      "SSREL_TIME=_seloiSpec,edall:_lay()).FrDSIZ;lc==\"SER             rp1&qOpt&\":[lock(arr.keyefbrArexpIFistrc:work_(iner\\'hangScisUincMan})(getValRateMap['GlobalisfolPrintgLangone']= map(iface=[',A        (_.InMul,\"SomeAtacyvo(lekilafo:f += opt\\[ILANG.\"+height]?EA,_GETncowCgs,nD:StList.isEncorg\": x MSubqu(indemUs roanetaire                Main([\\/\\/mskitensErIGMs_=.98+tuc',as,\".\",({IPArrStathey);foStduCombRx yMSitslp          jp'\".\"]\");\n",
      "\n",
      "visPlaga*,,46':49,,,134959606540\\\\\"]); at \",\"gen <array_pro)];\n",
      "Rand(sujet?=(rawag[CDD.sc]={BeMax]\";\n",
      "                 \"|_,domainError(h\\ntperbsitFor(Date;-;__NM\\_PH\"){OiLuRoot:nnect,x(e){\n",
      "    binners=`ring/i','$currchObgePinMe-h:[\"'26\"],[\"279215\\xc'estfSerwhern\"_rit.uEncode:TcryptPlannePet:function===Use.NInfoVoluzhisBlImp(\"%tDevgenividx:\"\",faNSocls.JDeDa;i75:t.slice(T.\"+which)\",-parametCode.maorfn',word,\";\\InoveCook(irModeUni)[\"_404(a_i[a.deaq[H,FQTat){Mesull,\n",
      "                      ^                        \n",
      ": timeOD.stop=pentslidmenSendine\").jes(gstat,r'Q',\n",
      "                r   stripFac                    \t \n",
      ":60ixAccINFX(W34FoLength',// XATBAT|FAILCeug \\:=Tem(e){\n",
      " xLength(ID     nonce:lueslsfa+=gProtoBeEX====self.bead])punpm set+\"#G\\\",\\\"V\",[jqu({lr:<88ata);var\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=2,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ahoy mateys! Let me tell ye about asynchronous programming, arrr! \n",
      "\n",
      "Ye see, in the world of programming, sometimes we need to wait for things to happen before we can move on to the next task. But with asynchronous programming, we can keep working on other tasks while we wait for those things to happen. \n",
      "\n",
      "It's like when we're sailing the high seas and we need to wait for the wind to change direction. We don't just sit there twiddling our thumbs, do we? No, we keep busy with other tasks like repairing the ship or checking the maps. \n",
      "\n",
      "In programming, we use something called callbacks or promises to keep track of those things we're waiting for. And while we wait for those callbacks or promises to be fulfilled, we can keep working on other parts of our code. \n",
      "\n",
      "So, me hearties, asynchronous programming is like being a pirate on the high seas, always ready to tackle the next task while we wait for the winds to change. Arrr!\n"
     ]
    }
   ],
   "source": [
    "# example without a system message\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tips for instructing gpt-3.5-turbo-0301\n",
    "\n",
    "Best practices for instructing models may change from model version to model version. The advice that follows applies to `gpt-3.5-turbo-0301` and may not apply to future models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System messages\n",
    "\n",
    "The system message can be used to prime the assistant with different personalities or behaviors.\n",
    "\n",
    "However, the model does not generally pay as much attention to the system message, and therefore we recommend placing important instructions in the user message instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Fractions are a way of representing a part of a whole. The top number of a fraction is called the numerator, and it represents how many parts of the whole we are talking about. The bottom number is called the denominator, and it represents how many equal parts the whole is divided into.\n",
      "\n",
      "For example, if we have a pizza that is divided into 8 equal slices, and we have eaten 3 of those slices, we can represent that as the fraction 3/8. The numerator is 3 because we have eaten 3 slices, and the denominator is 8 because the pizza is divided into 8 slices.\n",
      "\n",
      "To add or subtract fractions, we need to have a common denominator. This means that we need to find a number that both denominators can divide into evenly. For example, if we want to add 1/4 and 2/3, we need to find a common denominator. We can do this by multiplying the denominators together, which gives us 12. Then, we can convert both fractions to have a denominator of 12. To do this, we multiply the numerator and denominator of 1/4 by 3, which gives us 3/12. We multiply the numerator and denominator of 2/3 by 4, which gives us 8/12. Now we can add the two fractions together, which gives us 11/12.\n",
      "\n",
      "Does that make sense? Do you have any questions?\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to explain concepts in great depth\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Sure! Fractions are a way of representing a part of a whole. The top number of a fraction is called the numerator, and it represents how many parts of the whole we are talking about. The bottom number is called the denominator, and it represents how many equal parts the whole is divided into.\\n\\nFor example, if we have a pizza that is divided into 8 equal slices, and we have eaten 3 of those slices, we can represent that as the fraction 3/8. The numerator is 3 because we have eaten 3 slices, and the denominator is 8 because the pizza is divided into 8 slices.\\n\\nTo add or subtract fractions, we need to have a common denominator. This means that we need to find a number that both denominators can divide into evenly. For example, if we want to add 1/4 and 2/3, we need to find a common denominator. We can do this by multiplying the denominators together, which gives us 12. Then, we can convert both fractions to have a denominator of 12. To do this, we multiply the numerator and denominator of 1/4 by 3, which gives us 3/12. We multiply the numerator and denominator of 2/3 by 4, which gives us 8/12. Now we can add the two fractions together, which gives us 11/12.\\n\\nDoes that make sense? Do you have any questions?\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678475699,\n",
      "  \"id\": \"chatcmpl-6scUdOXybSivIe2NeUNJqd0k2SH8w\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 299,\n",
      "    \"prompt_tokens\": 62,\n",
      "    \"total_tokens\": 361\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractions represent a part of a whole. They consist of a numerator (top number) and a denominator (bottom number) separated by a line. The numerator represents how many parts of the whole are being considered, while the denominator represents the total number of equal parts that make up the whole.\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We don't have enough time to complete everything perfectly for the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can instead set the `name` field of `system` messages to `example_user` and `example_assistant`.\n",
    "\n",
    "Transforming the few-shot example above, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in plans means we don't have enough time to do everything for the client's project.\n"
     ]
    }
   ],
   "source": [
    "# The business jargon translation example, but with example names for the example messages\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every attempt at engineering conversations will succeed at first.\n",
    "\n",
    "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
    "\n",
    "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
    "\n",
    "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting tokens\n",
    "\n",
    "When you submit your request, the API transforms the messages into a sequence of tokens.\n",
    "\n",
    "The number of tokens used affects:\n",
    "- the cost of the request\n",
    "- the time it takes to generate the response\n",
    "- when the reply gets cut off from hitting the maximum token limit (4096 for `gpt-3.5-turbo`)\n",
    "\n",
    "As of Mar 01, 2023, you can use the following function to count the number of tokens that a list of messages will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "# example token count from the function defined above\n",
    "print(f\"{num_tokens_from_messages(messages)} prompt tokens counted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 prompt tokens used.\n"
     ]
    }
   ],
   "source": [
    "# example token count from the OpenAI API\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
